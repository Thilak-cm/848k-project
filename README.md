# 848K Course Project: Simplified GPT-2 with Advanced Mechanisms

This repository contains the implementation of a **simplified GPT-2** model, developed as part of the CMSC 848K course project. The goal of this project is to gain a deep understanding of GPT-2 by building it from scratch and experimenting with various attention mechanisms and positional embeddings.

## Overview

The project is inspired by Andrej Karpathy's "make more" series and explores:
- **Attention Mechanisms:** Efficient Attention, BigBird, Flash Attention
- **Positional Embeddings:** RoPE, Sinusoidal, FIRE/Kerpel, Learned

Key highlights:
- **Recreating iconic research papers:** For hands-on experience in implementing advanced NLP concepts.
- **Extensive logging:** Leveraging [Weights and Biases](https://wandb.ai/GPT2_848K/GPT%202%20848K%20Nexus%20Cluster?nw=nwuserthilakcm212) for tracking and comparing model performance. WandB has been a game changer for our comparative analysis, we couldn't have done this project without the brilliance that goes on under the hood of that python library
- **Comparative analysis:** Across different attention mechanisms and positional embeddings.

## Intentions and Goals

Our primary intentions for this project are:

- **Recreating techniques from scratch:**  
  To truly understand the intuitive concepts behind attention mechanisms and positional embeddings.

- **Exploring what makes them unique:**  
  Understanding the practical motivations and usefulness of these techniques in real-world scenarios.

- **Skill development:**
  - **PyTorch proficiency:** Gaining confidence and expertise in implementing models using PyTorch.
  - **Tensor manipulations:** Becoming comfortable with reshaping tensors and debugging dimension mismatches.
  - **Research implementation skills:** Learning how to recreate research-level models step-by-step.
