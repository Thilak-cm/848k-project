using device: cuda
C:\UMD\Sem 3\.venv\Lib\site-packages\torch\nn\modules\module.py:1160: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\c10/cuda/CUDAAllocatorConfig.h:28.)
  return t.to(
# Decayed parameter tensors: 50 with 124354560 parameters
# No Decay parameter tensors: 98 with 121344 parameters
Using Fused AdamW: True
Desired batch size: 524288, Gradient Accumulation Steps: 128
Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors
Loaded 338025 tokens
1 epoch = 82 iterations
C:\UMD\Sem 3\CMSC848K Multimodal Foundation Models\Project\848k-project\dataset\ShakesphereDataset.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  buf = torch.tensor(self.tokens[0][self.current_position:self.current_position + B*T + 1])
C:\UMD\Sem 3\CMSC848K Multimodal Foundation Models\Project\848k-project\models\GPT2FlashAttention.py:72: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # wow who knew flash attention was so easy to implement
Traceback (most recent call last):
  File "C:\UMD\Sem 3\CMSC848K Multimodal Foundation Models\Project\848k-project\gpt2_flash_sinusoidal.py", line 107, in <module>
    "embedding_size": model.config.n_embd,
                      ^^^^^^^^^^^^^^^^^^^
AttributeError: 'GPTConfig' object has no attribute 'n_embd'
