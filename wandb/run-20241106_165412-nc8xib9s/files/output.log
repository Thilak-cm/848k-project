using device: cuda
C:\UMD\Sem 3\.venv\Lib\site-packages\torch\nn\modules\module.py:1160: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\c10/cuda/CUDAAllocatorConfig.h:28.)
  return t.to(
# Decayed parameter tensors: 50 with 124354560 parameters
# No Decay parameter tensors: 98 with 121344 parameters
Using Fused AdamW: True
Desired batch size: 524288, Gradient Accumulation Steps: 128
Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors
Loaded 338025 tokens
1 epoch = 82 iterations
C:\UMD\Sem 3\CMSC848K Multimodal Foundation Models\Project\848k-project\dataset\ShakesphereDataset.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  buf = torch.tensor(self.tokens[0][self.current_position:self.current_position + B*T + 1])
C:\UMD\Sem 3\CMSC848K Multimodal Foundation Models\Project\848k-project\models\GPT2FlashAttention.py:72: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # wow who knew flash attention was so easy to implement
Epoch: 0, Loss: 10.957046508789062, lr: 5.9999999999999995e-05, norm: 7.655289649963379, Time Difference: 19717.30875968933ms, #tokens/sec: 26590.241416306795
C:\UMD\Sem 3\CMSC848K Multimodal Foundation Models\Project\848k-project\dataset\ShakesphereDataset.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  buf = torch.tensor(self.tokens[0][self.current_position:self.current_position + B*T + 1])
Epoch: 1, Loss: 9.677116394042969, lr: 0.00011999999999999999, norm: 5.021291255950928, Time Difference: 22530.086278915405ms, #tokens/sec: 23270.572225489017
Epoch: 2, Loss: 9.063630104064941, lr: 0.00017999999999999998, norm: 6.0418782234191895, Time Difference: 22716.1705493927ms, #tokens/sec: 23079.94645752545
Epoch: 3, Loss: 9.141548156738281, lr: 0.00023999999999999998, norm: 5.2204437255859375, Time Difference: 22270.21813392639ms, #tokens/sec: 23542.113366249476
Epoch: 4, Loss: 8.610698699951172, lr: 0.0003, norm: 4.000738143920898, Time Difference: 19994.967699050903ms, #tokens/sec: 26220.997597554822
Epoch: 5, Loss: 8.194538116455078, lr: 0.00035999999999999997, norm: 2.369662046432495, Time Difference: 22701.10011100769ms, #tokens/sec: 23095.268398282355
Epoch: 6, Loss: 7.768184661865234, lr: 0.00041999999999999996, norm: 2.7104814052581787, Time Difference: 23223.138570785522ms, #tokens/sec: 22576.104362549388
